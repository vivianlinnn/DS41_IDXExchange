{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "lETFivl3Xarv",
        "outputId": "2be0d75e-2c68-43fb-f17e-396f39a1cee4"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/processed/filtered_base.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3687234307.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mout_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/processed/preprocessed_v1.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mlist_price_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"ListPrice\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/processed/filtered_base.csv'"
          ]
        }
      ],
      "source": [
        "#Start:LC\n",
        "# Need to upload data to colab and change path\n",
        "in_path = 'processed_data/entire_dataset.csv'\n",
        "\n",
        "df = pd.read_csv(in_path, low_memory=False)\n",
        "list_price_cols = [col for col in df.columns if \"ListPrice\" in col]\n",
        "\n",
        "if list_price_cols:\n",
        "    df = df.drop(columns=list_price_cols)\n",
        "    print(\"Dropped list price columns:\", list_price_cols)\n",
        "# Drop columns >90% missing\n",
        "col_missing_pct = df.isna().mean()\n",
        "df = df.drop(columns=col_missing_pct[col_missing_pct > 0.90].index)\n",
        "\n",
        "# Numeric columns (optionally exclude target)\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "if \"ClosePrice\" in numeric_cols:\n",
        "    numeric_cols.remove(\"ClosePrice\")  # recommended: don't scale target\n",
        "\n",
        "# Median impute numeric\n",
        "medians = df[numeric_cols].median()\n",
        "df[numeric_cols] = df[numeric_cols].fillna(medians)\n",
        "\n",
        "# Standardize numeric (z-score)\n",
        "means = df[numeric_cols].mean()\n",
        "stds = df[numeric_cols].std(ddof=0).replace(0, 1)  # avoid divide-by-zero\n",
        "df[numeric_cols] = (df[numeric_cols] - means) / stds\n",
        "\n",
        "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "df.to_csv(out_path, index=False)\n",
        "print(\"Saved:\", out_path, \"shape:\", df.shape)\n",
        "\n",
        "# =====================================================\n",
        "# FEATURE SELECTION SECTION (NEW)\n",
        "# =====================================================\n",
        "\n",
        "# Remove constant features\n",
        "low_variance_cols = df[numeric_cols].columns[df[numeric_cols].std() == 0]\n",
        "numeric_cols = [c for c in numeric_cols if c not in low_variance_cols]\n",
        "df = df.drop(columns=low_variance_cols)\n",
        "\n",
        "print(\"Removed constant features:\", len(low_variance_cols))\n",
        "\n",
        "# Remove highly correlated features (>0.95)\n",
        "corr_matrix = df[numeric_cols].corr().abs()\n",
        "upper = corr_matrix.where(\n",
        "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "\n",
        "high_corr_cols = [\n",
        "    column for column in upper.columns\n",
        "    if any(upper[column] > 0.95)\n",
        "]\n",
        "\n",
        "df = df.drop(columns=high_corr_cols)\n",
        "numeric_cols = [c for c in numeric_cols if c not in high_corr_cols]\n",
        "\n",
        "print(\"Removed highly correlated features:\", len(high_corr_cols))\n",
        "\n",
        "# Rank features using Random Forest\n",
        "X = df[numeric_cols]\n",
        "y = df[\"ClosePrice\"]\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf.fit(X, y)\n",
        "\n",
        "importances = pd.Series(\n",
        "    rf.feature_importances_,\n",
        "    index=X.columns\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nTop 15 Important Features:\")\n",
        "print(importances.head(15))\n",
        "\n",
        "# Select top K features\n",
        "top_k = 20\n",
        "selected_features = importances.head(top_k).index.tolist()\n",
        "\n",
        "# Ensure location is always included\n",
        "for col in [\"Latitude\", \"Longitude\"]:\n",
        "    if col in df.columns and col not in selected_features:\n",
        "        selected_features.append(col)\n",
        "\n",
        "print(\"\\nFinal selected feature count:\", len(selected_features))\n",
        "\n",
        "# Keep only selected features + target\n",
        "df = df[selected_features + [\"ClosePrice\"]]\n",
        "\n",
        "# =====================================================\n",
        "\n",
        "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "df.to_csv(out_path, index=False)\n",
        "\n",
        "print(\"Saved:\", out_path, \"shape:\", df.shape)\n",
        "\n",
        "#End:LC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvGO71eaUrss",
        "outputId": "b4523cb9-75a1-4a3e-a514-51739b453b50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/r2/plrh1xlx4_b6wmq2hw3_92dw0000gn/T/ipykernel_5210/2864951107.py:20: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  monthly_data = pd.read_csv(f'california/CRMLSSold2025{month}.csv')\n"
          ]
        }
      ],
      "source": [
        "#BEGIN: Vivian Lin\n",
        "\n",
        "# Mount Google Drive for permanent storage of data\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Creating test set: December 2025\n",
        "test_set = pd.read_csv('california/CRMLSSold202512.csv')\n",
        "test_set = test_set[(test_set['PropertySubType'] == 'SingleFamilyResidence') & (test_set['PropertyType'] == 'Residential')]\n",
        "test_set.to_csv('processed_data/california_housing_test.csv')\n",
        "\n",
        "# Creating training set: April to November 2025 \n",
        "training_set = pd.DataFrame()\n",
        "\n",
        "for i in range(4, 12):\n",
        "    if i < 10:\n",
        "        month = '0' + str(i)\n",
        "    else:\n",
        "        month = str(i)\n",
        "    monthly_data = pd.read_csv(f'california/CRMLSSold2025{month}.csv')\n",
        "    monthly_data['month'] = month\n",
        "    training_set = pd.concat([training_set, monthly_data])\n",
        "training_set = training_set[(training_set['PropertySubType'] == 'SingleFamilyResidence') & (training_set['PropertyType'] == 'Residential')]\n",
        "training_set = training_set.reset_index(drop=True)\n",
        "\n",
        "training_set.to_csv('processed_data/california_housing_train.csv')\n",
        "\n",
        "#END: Vivian Lin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set['month'] = 12\n",
        "entire_dataset = pd.concat([training_set, test_set])\n",
        "entire_dataset.to_csv('entire_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
